{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio.transforms as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchaudio\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import pyaudio\n",
    "from dataclasses import dataclass\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "import torch\n",
    "\n",
    "\n",
    "from djitellopy import Tello\n",
    "import wave\n",
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Define the root directory and the target classes\n",
    "# root_dir = '/Users/luccaemmanuel/Desktop/BIA/FunctionCalling/Dados/Dados_wav_augmented'\n",
    "# classes = ['DESCER', 'DIREITA', 'ESQUERDA', 'FRENTE', 'SUBIR', 'TRAS']\n",
    "\n",
    "# # Create the class directories if they don't exist\n",
    "# for class_name in classes:\n",
    "#     class_path = os.path.join(root_dir, class_name)\n",
    "#     if not os.path.exists(class_path):\n",
    "#         os.makedirs(class_path)\n",
    "\n",
    "# # Iterate over each subdirectory in the root directory\n",
    "# for subdir in os.listdir(root_dir):\n",
    "#     subdir_path = os.path.join(root_dir, subdir)\n",
    "#     if os.path.isdir(subdir_path):\n",
    "#         # Iterate over each class directory in the subdirectory\n",
    "#         for class_name in classes:\n",
    "#             class_subdir_path = os.path.join(subdir_path, class_name)\n",
    "#             if os.path.isdir(class_subdir_path):\n",
    "#                 # Move all files from the class subdirectory to the corresponding target class directory\n",
    "#                 for file_name in os.listdir(class_subdir_path):\n",
    "#                     source_path = os.path.join(class_subdir_path, file_name)\n",
    "#                     target_path = os.path.join(root_dir, class_name, file_name)\n",
    "#                     shutil.move(source_path, target_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = pyaudio.PyAudio()\n",
    "for i in range(p.get_device_count()):\n",
    "    info = p.get_device_info_by_index(i)\n",
    "    print(f\"Device {i}: {info['name']}\")\n",
    "p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecordVoice:\n",
    "    def __init__(self, \n",
    "                 chunk=1024, \n",
    "                 format=pyaudio.paInt16, \n",
    "                 channels=1, \n",
    "                 rate=44100, \n",
    "                 record_seconds=2.5,\n",
    "                 device_index=4):\n",
    "        \n",
    "        self.chunk = chunk\n",
    "        self.format = format\n",
    "        self.channels = channels\n",
    "        self.rate = rate\n",
    "        self.record_seconds = record_seconds\n",
    "        self.device_index = device_index\n",
    "        self.frames = []\n",
    "        self.p = pyaudio.PyAudio()\n",
    "\n",
    "    def record(self,output_filename):\n",
    "        stream = self.p.open(format=self.format,\n",
    "                             channels=self.channels,\n",
    "                             rate=self.rate,\n",
    "                             input=True,\n",
    "                             input_device_index=self.device_index,\n",
    "                             frames_per_buffer=self.chunk)\n",
    "\n",
    "        print(\"* recording\")\n",
    "\n",
    "        for i in range(0, int(self.rate / self.chunk * self.record_seconds)):\n",
    "            data = stream.read(self.chunk)\n",
    "            self.frames.append(data)\n",
    "\n",
    "        print(\"* done recording\")\n",
    "\n",
    "        stream.stop_stream()\n",
    "        stream.close()\n",
    "        self.p.terminate()\n",
    "\n",
    "        wf = wave.open(output_filename, 'wb')\n",
    "        wf.setnchannels(self.channels)\n",
    "        wf.setsampwidth(self.p.get_sample_size(self.format))\n",
    "        wf.setframerate(self.rate)\n",
    "        wf.writeframes(b''.join(self.frames))\n",
    "        wf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"/Users/luccaemmanuel/Desktop/BIA/FunctionCalling/models/asr_processor_augmented\"\n",
    "\n",
    "w2v_processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "#w2v_processor.save_pretrained(\"./wav2vec2-large-xlsr-53-portuguese\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH=30000\n",
    "\n",
    "def pad_or_truncate(waveform):\n",
    "        if waveform.shape[1] > MAX_LENGTH:\n",
    "            waveform = waveform[:, :MAX_LENGTH]\n",
    "        else:\n",
    "            pad_length = MAX_LENGTH - waveform.shape[1]\n",
    "            waveform = torch.nn.functional.pad(waveform, (0, pad_length))\n",
    "        return waveform\n",
    "    \n",
    "def process_audio(audio_path):\n",
    "    resampler = T.Resample(orig_freq=48000, new_freq=16000)\n",
    "\n",
    "    waveform, sample_rate = torchaudio.load(audio_path)\n",
    "    if sample_rate != 16000:\n",
    "        waveform = resampler(waveform)\n",
    "    waveform = pad_or_truncate(waveform)\n",
    "    input_values = w2v_processor(waveform.squeeze().numpy(), sampling_rate=16000, return_tensors='pt').input_values\n",
    "    return input_values, waveform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(1, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=5)\n",
    "        self.conv3 = nn.Conv1d(64, 128, kernel_size=5)\n",
    "\n",
    "        self.fc1 = nn.Linear(39808, 128)  # Adjust the dimension based on your input size\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 16)\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool1d(x, kernel_size=4)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool1d(x, kernel_size=4)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool1d(x, kernel_size=6)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "        return output1, output2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(1, 64, kernel_size=10)\n",
    "        self.conv2 = nn.Conv1d(64, 128, kernel_size=10)\n",
    "        self.conv3 = nn.Conv1d(128, 256, kernel_size=10)\n",
    "\n",
    "\n",
    "        self.fc1 = nn.Linear(52736, 512)  # Adjust the dimension based on your input size\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 32)\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool1d(x, kernel_size=4)  # Increase pooling size\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool1d(x, kernel_size=6)  # Increase pooling size\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool1d(x, kernel_size=6)  # Increase pooling size\n",
    "        x = x.view(x.size()[0], -1)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "        return output1, output2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(1, 128, kernel_size=10)  # Increased the number of filters\n",
    "        self.conv2 = nn.Conv1d(128, 256, kernel_size=10)  # Increased the number of filters\n",
    "        self.conv3 = nn.Conv1d(256, 512, kernel_size=10)  # Increased the number of filters\n",
    "        self.conv4 = nn.Conv1d(512, 1024, kernel_size=10)  # Added an additional convolutional layer\n",
    "\n",
    "        self.fc1 = nn.Linear(32768, 1024)  # Adjust the dimension based on your input size\n",
    "        self.fc2 = nn.Linear(1024, 512)  # Increased the number of neurons\n",
    "        self.fc3 = nn.Linear(512, 256)  # Increased the number of neurons\n",
    "        self.fc4 = nn.Linear(256, 32)  # Added an additional fully connected layer\n",
    "\n",
    "    def forward_once(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool1d(x, kernel_size=4)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool1d(x, kernel_size=6)\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.max_pool1d(x, kernel_size=6)\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.max_pool1d(x, kernel_size=6)\n",
    "        x = x.view(x.size()[0], -1)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        output1 = self.forward_once(input1)\n",
    "        output2 = self.forward_once(input2)\n",
    "        return output1, output2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model\n",
    "model = SiameseNetwork().to('mps')\n",
    "model_path = '/Users/luccaemmanuel/Desktop/BIA/FunctionCalling/models/pipeline3_m2_pipe1asr_mini.pth'\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device('mps')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class VectorDatabase:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.data = np.array([])  # Encoded data\n",
    "        self.labels = []  # Corresponding labels\n",
    "\n",
    "    def encode_database(self, path,new_base_path):\n",
    "        '''\n",
    "        Receives a path to a folder containing wav files\n",
    "        Encodes the audio files and saves them in a new folder\n",
    "        '''\n",
    "        if not os.path.exists(new_base_path):\n",
    "            os.makedirs(new_base_path)\n",
    "        \n",
    "        for root, _, files in os.walk(path):\n",
    "            for file in tqdm(files):\n",
    "                if file.endswith('.wav'):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    encoded_data = self.encode(file_path)\n",
    "                    \n",
    "                    # Create new folder structure in the encoded folder\n",
    "                    relative_path = os.path.relpath(root, path)\n",
    "                    new_dir = os.path.join(new_base_path, relative_path)\n",
    "                    if not os.path.exists(new_dir):\n",
    "                        os.makedirs(new_dir)\n",
    "                    \n",
    "                    # Save the encoded numpy array\n",
    "                    new_file_path = os.path.join(new_dir, file.replace('.wav', '.npy'))\n",
    "                    np.save(new_file_path, encoded_data)\n",
    "\n",
    "    def encode(self, path):\n",
    "        input_values, waveform = process_audio(path)\n",
    "        input_tensor = input_values.unsqueeze(0)  # Add batch dimension\n",
    "        self.model.eval()\n",
    "        input_tensor = input_tensor.to('mps')\n",
    "        with torch.no_grad():\n",
    "            output, _ = self.model(input_tensor, input_tensor)  # Duplicate input for the second branch\n",
    "        output = output.cpu().numpy()[0]\n",
    "        return output\n",
    "        \n",
    "    def load_database(self, path):\n",
    "        '''\n",
    "        Receives a path to a folder containing encoded files\n",
    "        Loads the data in dictionary\n",
    "        '''\n",
    "        data = []\n",
    "        labels = []\n",
    "        \n",
    "        for root, _, files in os.walk(path):\n",
    "            for file in files:\n",
    "                if file.endswith('.npy'):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    encoded_data = np.load(file_path)\n",
    "                    \n",
    "                    # Extract the label from the directory name\n",
    "                    label = os.path.basename(root)\n",
    "                    \n",
    "                    data.append(encoded_data)\n",
    "                    labels.append(label)\n",
    "        \n",
    "        self.data = np.array(data)\n",
    "        self.labels = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = VectorDatabase(model)\n",
    "data_to_augment = '/Users/luccaemmanuel/Desktop/BIA/FunctionCalling/Dados/Dados_wav_augmented'\n",
    "new_base_path = '/Users/luccaemmanuel/Desktop/BIA/FunctionCalling/Dados/Dados_wav_augmented_encoded_mini_model'\n",
    "test.encode_database(data_to_augment,new_base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = VectorDatabase(model)\n",
    "test.encode('/Users/luccaemmanuel/Desktop/BIA/FunctionCalling/voice.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KNNModel:\n",
    "    def __init__(self, vector_database, n=35):\n",
    "        self.vector_database = vector_database\n",
    "        self.data = vector_database.data\n",
    "        self.labels = vector_database.labels\n",
    "        \n",
    "        self.knn = NearestNeighbors(n_neighbors=n, metric='euclidean')\n",
    "        self.knn.fit(self.data)\n",
    "\n",
    "    def search(self, query_path):\n",
    "        '''\n",
    "        Receives a query path\n",
    "        Returns the labels of the most similar vectors\n",
    "        '''\n",
    "        # Encode the query audio\n",
    "        if isinstance(query_path,str):\n",
    "            query_vector = self.vector_database.encode(query_path).reshape(1, -1)\n",
    "        elif isinstance(query_path,np.ndarray):\n",
    "            query_vector = query_path.reshape(1,-1)\n",
    "        \n",
    "        # Find the n nearest neighbors\n",
    "        distances, indices = self.knn.kneighbors(query_vector)\n",
    "        \n",
    "        # Get the labels of the nearest neighbors\n",
    "        nearest_labels = [self.labels[i] for i in indices[0]]\n",
    "        \n",
    "        return nearest_labels\n",
    "\n",
    "    def make_inference(self, query_path):\n",
    "        '''\n",
    "        Performs k-neighbors to make an inference\n",
    "        Count the majority of the neighbors and return an inference\n",
    "        '''\n",
    "        search_results = self.search(query_path)\n",
    "        count = {}\n",
    "        \n",
    "        for label in search_results:\n",
    "            count[label] = count.get(label, 0) + 1\n",
    "        \n",
    "        return max(count, key=count.get)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "class KNNModel:\n",
    "    def __init__(self, vector_database, n=1, test_size=0.2, random_state=42):\n",
    "        self.vector_database = vector_database\n",
    "        self.data = vector_database.data\n",
    "        self.labels = vector_database.labels\n",
    "\n",
    "        # Split data into training and testing sets\n",
    "        self.train_data, self.test_data, self.train_labels, self.test_labels = train_test_split(\n",
    "            self.data, self.labels, test_size=test_size, random_state=random_state\n",
    "        )\n",
    "\n",
    "        self.knn = KNeighborsClassifier(n_neighbors=n, weights='distance', metric='euclidean')\n",
    "        self.knn.fit(self.train_data, self.train_labels)\n",
    "\n",
    "    def search(self, query_path):\n",
    "        '''\n",
    "        Receives a query path\n",
    "        Returns the labels of the most similar vectors\n",
    "        '''\n",
    "        # Encode the query audio\n",
    "        if isinstance(query_path, str):\n",
    "            query_vector = self.vector_database.encode(query_path).reshape(1, -1)\n",
    "        elif isinstance(query_path, np.ndarray):\n",
    "            query_vector = query_path.reshape(1, -1)\n",
    "\n",
    "        # Find the n nearest neighbors\n",
    "        distances, indices = self.knn.kneighbors(query_vector, return_distance=True)\n",
    "\n",
    "        # Get the labels of the nearest neighbors\n",
    "        nearest_labels = [self.train_labels[i] for i in indices[0]]\n",
    "\n",
    "        return nearest_labels\n",
    "\n",
    "    def make_inference(self, query_path,print_count = False):\n",
    "        '''\n",
    "        Performs k-neighbors to make an inference\n",
    "        Count the majority of the neighbors and return an inference\n",
    "        '''\n",
    "        search_results = self.search(query_path)\n",
    "        count = {}\n",
    "\n",
    "        for label in search_results:\n",
    "            count[label] = count.get(label, 0) + 1\n",
    "\n",
    "        if print_count:\n",
    "            print(count)\n",
    "        return max(count, key=count.get)\n",
    "\n",
    "    def test_model(self):\n",
    "        '''\n",
    "        Tests the model on the test data and returns a classification report\n",
    "        '''\n",
    "        predictions = self.knn.predict(self.test_data)\n",
    "        report = classification_report(self.test_labels, predictions)\n",
    "        return report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_database = VectorDatabase(model)\n",
    "vector_database.load_database('/Users/luccaemmanuel/Desktop/BIA/FunctionCalling/Dados/Dados_wav_augmented_encoded_mini_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNNModel(vector_database,n=15)\n",
    "knn.make_inference('/Users/luccaemmanuel/Desktop/BIA/FunctionCalling/voice.wav',print_count = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in range(1,30,2):\n",
    "    print('N: ',n)\n",
    "    knn = KNNModel(vector_database, n)\n",
    "    print(knn.test_model())\n",
    "    print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drone "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DroneController:\n",
    "    def __init__(self,drone):\n",
    "        self.drone = drone\n",
    "        \n",
    "    def control_drone(self, command):\n",
    "        if command == \"SUBIR\":\n",
    "            self.drone.takeoff()\n",
    "            self.drone.move_up(70)\n",
    "        elif command == \"DESCER\":\n",
    "            self.drone.move_down(70)\n",
    "            self.drone.land()\n",
    "        elif command == \"ESQUERDA\":\n",
    "            self.drone.move_left(70)\n",
    "        elif command == \"DIREITA\":\n",
    "            self.drone.move_right(70)\n",
    "        elif command == \"FRENTE\":\n",
    "            self.drone.move_forward(70)\n",
    "        elif command == \"TRAS\":\n",
    "            self.drone.move_back(70)\n",
    "        else:\n",
    "            print(\"Unknown command\")\n",
    "\n",
    "        self.drone.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drone = Tello()\n",
    "#drone.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recorder = RecordVoice()\n",
    "vector_database = VectorDatabase(model)\n",
    "vector_database.load_database('/Users/luccaemmanuel/Desktop/BIA/FunctionCalling/Dados/Dados_wav_encoded')\n",
    "knn = KNNModel(vector_database)\n",
    "controller = DroneController(drone)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/Users/luccaemmanuel/Desktop/BIA/FunctionCalling/voice.wav'\n",
    "recorder = RecordVoice()\n",
    "recorder.record(path)\n",
    "knn_text = knn.make_inference(path, 10)\n",
    "print('Commmand: ',knn_text)\n",
    "#controller.control_drone(knn_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_database = VectorDatabase(model)\n",
    "vector_database.load_database('/Users/luccaemmanuel/Desktop/BIA/FunctionCalling/Dados/Dados_wav_augmented_encoded_mini_model')\n",
    "knn = KNNModel(vector_database,n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import time\n",
    "base_directory = '/Users/luccaemmanuel/Desktop/BIA/FunctionCalling/Dados/Dados_wav_augmented'\n",
    "predictions = []\n",
    "labels = []\n",
    "inference_times = []\n",
    "for label in os.listdir(base_directory):\n",
    "    folder_path = os.path.join(base_directory, label)\n",
    "    if os.path.isdir(folder_path):\n",
    "        for audio_file in tqdm(glob.glob(os.path.join(folder_path, '*.wav'))):\n",
    "            # calculate time of inference\n",
    "            start = time.time()\n",
    "            \n",
    "            prediction = knn.make_inference(audio_file)\n",
    "            predictions.append(prediction)\n",
    "            \n",
    "            end = time.time()\n",
    "            inference_times.append(end - start)\n",
    "            labels.append(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute report\n",
    "report = classification_report(labels, predictions)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n",
    "\n",
    "print('Mean inference time: ', np.mean(inference_times))\n",
    "print('Std inference time: ', np.std(inference_times))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3:\n",
    "    accuracy: 0.92\n",
    "\n",
    "n = 5\n",
    "    accuracy: 0.84\n",
    "\n",
    "n = 35 \n",
    "    accuracy: 0.48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "f_call_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
